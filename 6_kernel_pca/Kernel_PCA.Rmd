---
title: "Kernel PCA"
author: "Bealy MECH"
date: "12/4/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ecercise 1 : Kernel Principal Component Analysis

```{r}
myKPCA <- function(X, k=2, kernel="Gaussien", beta=1){
  if (kernel == "Gaussien"){
    K <- exp(-1/beta*(as.matrix(dist(X))^2))
  } else {
    K <- X%*%t(X)
  }
  
  # Centering 
  n <- nrow(X)
  II <- matrix(1/n,n,n)
  Ktilde <- K - 2*II%*%K + II%*%K%*%II
  
  # Eigenvalue decomposition
  res <- eigen(Ktilde)
  alpha <- res$vectors
  lambda <- res$values
  
  # Projection
  Y <- K%*%alpha[,1:k]
  return(list(Y=Y, lambda=lambda[1:k]))
}
```

```{r}
myKPCA<-function(X,k=2,kernel="Gaussian",beta=1){
  X<-as.matrix(X)
  if (kernel == "Gaussian") 
      {K<- exp(-1/beta*(as.matrix(dist(X))^2))
  } else {
    K<-X%*%t(X)
    }
  
  # Centering
  n<-nrow(X)
  II<-matrix(1/n,n,n)
  Ktilde<-K -2*II %*% K+ II %*%K%*%II
  
  # Eigenvalue decomposition
  res<-svd(Ktilde)
  alpha<-res$u
  lambda<-res$d^2
  
  # Projection
  Y<-Ktilde%*%alpha[,1:k]
  return(list(Y=Y,lambda=lambda[1:k]))
}
  
data(iris)
X<-iris[,1:4]

# KPCA
myKPCA(scale(X,center=TRUE,scale = TRUE),beta=2)->results
plot(results$Y[,1],results$Y[,2],col=iris$Species)
  
# PCA
Y<-princomp(X,cor = TRUE)$scores[,1:2]
plot(Y[,1],Y[,2],col=iris$Species)


# KPCA with linear kernel
myKPCA(scale(X,center=TRUE,scale = TRUE),kernel="Linear")->results
plot(results$Y[,1],results$Y[,2],col=iris$Species)

```

```{r}
library(kernlab)
data(spam)
dim(spam)
head(spam)
```



# Exercise 2 : Spectral Clustering

```{r}
library(mlbench)

set.seed(111)
obj <- mlbench.spirals(100,1,0.025)
myData <- data.frame(4 * obj$x)
names(myData) <- c("X1","X2")
plot(myData, col = "red")

myData <- as.matrix(myData)
dim(myData)
head(myData)
```

```{r}
library(mlbench)

set.seed(111)
obj <- mlbench.spirals(100,1,0.025)
my.data <-  data.frame(4 * obj$x)
names(my.data)<-c("X1","X2")
par(mfrow=c(1,2))
plot(my.data,col=c('orange','blue')[obj$classes],main="Original Classes")
my.data<-as.matrix(my.data)
plot(my.data,col=c('orange','blue')[kmeans(my.data,2)$cluster],main="Kmeans")
```

```{r}
Distance.mat <- as.matrix(dist(my.data))
image(Distance.mat)
diag(Distance.mat) <- max(Distance.mat)
A <- apply(Distance.mat,1,function(x){
  nearest.neig <- which.min(x)
  x[nearest.neig] <- 1
  x[-nearest.neig] <- 0
  return(x)
})
A <- (A + t(A))>0
image(A)
```

```{r}
library(igraph)
plot(graph_from_adjacency_matrix(A, mode = "undirected"))
```

```{r}
diag(A)<-0
D<-diag(colSums(A))
L<-D-A
color.kmeans<-kmeans(eigen(L)$vectors[,97:100],2,nstart = 30)$cluster
par(mfrow=c(1,3))
plot(my.data,col=c('orange','blue')[obj$classes],main="Original Classes")
plot(my.data,col=c('orange','blue')[kmeans(my.data,2)$cluster],main="Kmeans")
plot(my.data,col=c('orange','blue')[color.kmeans],main="Spectral Kmeans")
```

2. Compute $K$ the matrix of similarities for this dataset using the gaussian kernel

```{r}
sigma2 <- 1
K <- exp(-as.matrix(dist(my.data))^2 / sigma2)
dim(K)
```

3. The next step consists in computing an affinity matrix $A$ based on $K$. A must be made of positive values and be symmetric. This is usually done by applying a k-nearest nighboor filter to build a representation of a graph connecting just the closest dataset points. However, to be symmetric, if $A_{ij}$ is selected as a nearest neighboor, so will $A_{ji}$:

```{r}
A <- K>0.5
diag(A) <- 0
D <- diag(colSums(A))
L <- D-A
color.kmeans <- kmeans(eigen(L)$vectors[,97:100],2,nstart = 30)$cluster
par(mfrow=c(1,3))
plot(my.data,col=c('orange','blue')[obj$classes],main="Original Classes")
plot(my.data,col=c('orange','blue')[kmeans(my.data,2)$cluster],main="Kmeans")
plot(my.data,col=c('orange','blue')[color.kmeans],main="Spectral Kmeans")
```

```{r}
Distance.mat <- as.matrix(dist(my.data))
image(Distance.mat)
diag(Distance.mat) <- max(Distance.mat)
A <- apply(Distance.mat,1,function(x){
  nearest.neig <- rank(x)[1:3]
  x[nearest.neig] <- 1
  x[-nearest.neig] <- 0
  return(x)
})
A <- (A + t(A))>0
image(A)
```


```{r}
library(igraph)
plot(graph_from_adjacency_matrix(A, mode = "undirected"))
```

```{r}
diag(A)<-0
D<-diag(colSums(A))
L<-D-A
color.kmeans<-kmeans(eigen(L)$vectors[,98:100],2,nstart = 30)$cluster
par(mfrow=c(1,3))
plot(my.data,col=c('orange','blue')[obj$classes],main="Original Classes")
plot(my.data,col=c('orange','blue')[kmeans(my.data,2)$cluster],main="Kmeans")
plot(my.data,col=c('orange','blue')[color.kmeans],main="Spectral Kmeans")
```

# Examples 


## Normalized crabs

```{r, echo=FALSE}
library(MASS)
n=dim(crabs)[1] # nb d'individus
crabsquant<-crabs[,4:8]
n=dim(crabs)[1] # nb d'individus
# gardons les variables quantitatives
crabsquant<-crabs[,4:8]
# Faisons une simple petite transformation pour enlever l'effet taille
crabsquant2<-(crabsquant/crabsquant[,3])[,-3]
# mettons les noms ? jour
j=0
for(i in c(1,2,4,5)) 
{
 j=j+1
 names(crabsquant2)[j]<-c(paste(names(crabsquant)[i],"/",names(crabsquant[3])))
}
true.classes<- paste(crabs$sp,crabs$sex,sep="-")
```


## Hierarchical agglomerative clustering with R base

```{r}
dist_mat<-dist(crabsquant2)
hclust_ward.D2 <- hclust(dist_mat, method = 'ward.D2')
plot(hclust_ward.D2)
```



## Cutting the tree

to get a partition

```{r}
cut_ward.D2 <- cutree(hclust_ward.D2,k = 4)
table(true.classes,cut_ward.D2)
```

## Display partition in dendrogram

```{r}
plot(hclust_ward.D2,labels = true.classes,cex=.5)
rect.hclust(hclust_ward.D2 , k = 4, border = 2:6)
```

##  Using package "dendextend"

```{r}
suppressPackageStartupMessages(library(dendextend))
ward.D2_dend_obj <- as.dendrogram(hclust_ward.D2)
ward.D2_col_dend <- color_branches(ward.D2_dend_obj, k = 4)
plot(ward.D2_col_dend)
```

## Biplot of the hierachical clutering for 4 clusters

```{r}
suppressPackageStartupMessages(library(ggplot2))
ggplot(crabsquant2, aes(x=`FL / CL`, y = `RW / CL`, color = factor(cut_ward.D2))) + geom_point()
```


##  Dertermining and Visualizing the Optimal Number of Clusters

 Silhouette

```{r}
library(factoextra)
fviz_nbclust(as.matrix(dist_mat), FUN = hcut, method = "silhouette")
```

##  Dertermining and Visualizing the Optimal Number of Clusters

 Gap Statistics

```{r}
gap_stat <- cluster::clusGap(crabsquant2, FUN = hcut, K.max = 10, B = 10)
fviz_gap_stat(gap_stat)
```


##  Dertermining and Visualizing the Optimal Number of Clusters

### WSS

```{r}
library(factoextra)
fviz_nbclust(as.matrix(dist_mat), FUN = hcut, method = "wss")
```

## Hierarchical agglomerative clustering with R package cluster

### agnes

> Agglomerative Nesting
> Description:
> Computes agglomerative hierarchical clustering of the dataset.
>Usage:
> agnes(x, diss = inherits(x, "dist"),
>       metric = "euclidean", stand = FALSE, method = "average",
>       keep.diss = n < 100, keep.data = !diss)

## Agglomerative clustering with Ward

```{r echo=TRUE}
library(cluster)
res<-agnes(crabsquant2,method="ward")
```

## Banner plot

```{r echo=TRUE,fig=TRUE}
plot(res,which.plots=1)
```

## Dendogram 

```{r echo=TRUE,fig=TRUE}
plot(res,which.plots=2)
```

## Levels of fusion

```{r echo=TRUE,fig=TRUE}
plot(1:199,sort(res$height))
```


## Divisive Hiearchical Clustering with R 

### `diana`

>DIvisive ANAlysis Clustering
>Description:
> Computes a divisive hierarchical clustering of the dataset
> returning an object of class 'diana'.
>Usage:
>
> diana(x, diss = inherits(x, "dist"), metric = "euclidean", stand = >FALSE,
>       keep.diss = n < 100, keep.data = !diss)
```

# Another questions from the lesson

Load the iris data set and try hierarchical clustering with all available fusion strategies. Compare the 2 and 3 classes clustering of each fusion criterion with true classes.

```{r}
library(MASS)
data(iris)
dim(iris)
```
```{r}
iris4<-iris[,1:4]
hclust_ward.D2 <- hclust(dist(iris4), method = 'ward.D2')
plot(hclust_ward.D2)

true.classes<-iris$Species
cut_ward.D2 <- cutree(hclust_ward.D2,k = 3)
table(true.classes,cut_ward.D2)
```

```{r}
iris4<-iris[,1:4]
hclust_ward.D <- hclust(dist(iris4), method = 'ward.D')
plot(hclust_ward.D)

true.classes<-iris$Species
cut_ward.D <- cutree(hclust_ward.D,k = 3)
table(true.classes,cut_ward.D) 
```

```{r}
pairs(iris4, col=iris$Species)
```

```{r}
hclust_single <- hclust(dist(iris4), method = 'single') # single link => min
plot(hclust_single)

true.classes<-iris$Species
cut_single <- cutree(hclust_single,k = 3)
table(true.classes,cut_single) # almost only 2 classes
```

```{r}
hclust_complete <- hclust(dist(iris4), method = 'complete')
plot(hclust_complete)

true.classes<-iris$Species
cut_complete <- cutree(hclust_complete,k = 3)
table(true.classes,cut_complete)
```

```{r}
hclust_average <- hclust(dist(iris4), method = 'average')
plot(hclust_average)

true.classes<-iris$Species
cut_average <- cutree(hclust_average,k = 3)
table(true.classes,cut_average)
```

# Heatmap

Try heatmap to cluster both rows and columns and then display the results

```{r}
heatmap(as.matrix(iris4), hclustfun = function(x) hclust(x, method="ward.D"))
```

```{r}
heatmap(as.matrix(iris4), hclustfun = function(x) hclust(x, method="ward.D2"))
```

3. Choose the number of classes using Ward.D and silhouette algorithm.

```{r}
library(factoextra)
library(ggplot2)
fviz_nbclust(as.matrix(dist(iris4)), FUNcluster = hcut, method = "wss", hc_method = "ward.D")
fviz_nbclust(as.matrix(dist(iris4)), FUNcluster = hcut, method = "silhouette", hc_method = "ward.D")
```


### Partition Around Medoids

The variant of k-means when data is available as dissimilarity matrix.

```{r}
library(cluster)
x <- rbind(cbind(rnorm(10,0,0.5), rnorm(10,0,0.5)),
           cbind(rnorm(15,5,0.5), rnorm(15,5,0.5)))
plot(x)
```
```{r}
pamx <- pam(x,2)
pamx 
summary(pamx)
```

# SBM Model

Algorithm of simulation:

Write an R function to simulate an affiliation network (special case of stochastic block model)

```{r}
class.ind<-function (cl)
{ 
    n <- length(cl)
    cl <- as.factor(cl)
    x <- matrix(0, n, length(levels(cl)))
    x[(1:n) + n * (unclass(cl) - 1)] <- 1
    dimnames(x) <- list(names(cl), levels(cl))
    x
}

class.ind(c(1,1,1,2,2))
```

```{r}
graph.affiliation<-function(n=100,Pi=c(1/2,1/2),alpha=0.7,beta=0.05) {
      # INPUT  n: number of vertex
      #           Pi : vecteur of class proportion
      #           alpha: proba of edge given  same class
      #           beta: proba of edge given two different classes
      # OUTPUT x: adjacency matrix
      #        cluster: class vector
      #           
     
      X<-matrix(0,n,n); # reserve space for adjacency matrix
      Q<-length(Pi);
      rmultinom(1, size=n, prob = Pi)->nq;
      Z<-class.ind(rep(1:Q,nq));
      Z<-Z[sample(1:n,n),];
      for (i in 1:n)
        for (j in i:n)
            {
            # if i and j in same class
            if (which.max(Z[i,])  == which.max(Z[j,])) p<-alpha else  p<-beta
            if ((rbinom(1,1,p))&(i != j)) {X[i,j]<-1; X[j,i]<-1}
            }
       return(list(X=X,cluster=apply(Z,1,which.max)) )   
  }

mygraph<-graph.affiliation(alpha=0.7,beta=0.05)
library(igraph)
plot(graph_from_adjacency_matrix(mygraph$X,mode="undirected"),vertex.color=mygraph$cluster)
```

```{r}
matrix(0,5,7)
```